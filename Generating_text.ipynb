{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the text of Anna Karenina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "Passing one character at a time into an rnn.\n",
    "One hot encod. This is then fed into a hidden layer\n",
    "the hidden layer has 2 outputs: an output and a hidden state\n",
    "The output goes to a final fully connected output layer which produces class scores in which we can apply a softmax function to produce the probability for the most likely next character\n",
    "\n",
    "\n",
    "At the end the network is going to be able to generate new text, one character at a time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deep-learning-v2-pytorch/recurrent-neural-networks/char-rnn/data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the text into numerical tokens because the network\n",
    "# can only learn from numerical data\n",
    "\n",
    "# creating a unique vocab\n",
    "chars = tuple(set(text))\n",
    "\n",
    "# mapping every character to a unique integer\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# getting a dictionary that goes from integers to characters\n",
    "char2int = {}\n",
    "for key, value in int2char.items():\n",
    "    char2int[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "# get all the encoded words and add to the encoded list\n",
    "encoded = []\n",
    "for char in text[:15]:\n",
    "    encoded.append(char2int[char])\n",
    "    \n",
    "encoded = np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_arr, n_labels):\n",
    "    '''Takes in an encoded array and turns it into an encoded vector\n",
    "    of a specific length'''\n",
    "    # # initialize an array with zeros with \n",
    "    # columns=n_labels\n",
    "    # rows for each character in the encoded arr = columns in the encoded array\n",
    "    \n",
    "    initial_arr = np.zeros([encoded_arr.shape[1], n_labels])\n",
    "    \n",
    "    # fill out the elements present in the encoded_arr with ones\n",
    "    # position = \n",
    "    # array[row to add 1, column to add 1]\n",
    "    initial_arr[np.arange(encoded_arr.shape[1]), encoded_arr.flatten()] = 1\n",
    "    \n",
    "    # reshaping to get the original array\n",
    "    # adding another dimensionality\n",
    "    one_hot = initial_arr.reshape((*encoded_arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(1,20,1)\n",
    "batch_size = 2\n",
    "seq_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_total = batch_size * seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(arr)//batch_size_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr[:n_batches * batch_size_total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    \n",
    "    '''Create a generator that returns batches of size \n",
    "    batch_size x seq_length from arr.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    arr: Array you want to make batches from\n",
    "    batch_size: batch size, the number of sequences per batch\n",
    "    seq_length: num of encoded chars in a sequence'''\n",
    "    \n",
    "    # calculating the num of characters in a mini batch\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    \n",
    "    # num of complete batches that we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    \n",
    "    # keep only enough characters to make full batches\n",
    "    # some data may be lost here but generally it doesnt really matter\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    \n",
    "    \n",
    "    # reshape into batch_size rows\n",
    "    # the -1 is just a dimension placeholder. It will automatically fill\n",
    "    # up the second dimension to wathever size it needs to be\n",
    "    # to accomodate all the data\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_lenghth):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        \n",
    "        # the targets shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try: \n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y\n",
    "                   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('No GPU available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "class rnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        \n",
    "        supper().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # define the layers of the model\n",
    "        # len(self.chars: len of the one hot encoded input character)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, droupout=drop_prob,\n",
    "                           batch_size=True)\n",
    "        self.dropout = nn.Droupout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # an lstm has a hidden and a cell state that are saved as tuple\n",
    "        # in this function we are initializing the hidden weights to zero        \n",
    "        r_out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_out)\n",
    "        out = out.view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                     weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "                \n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-215-a28670658595>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-215-a28670658595>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    def train(model, data, epochs=10, batch_size=10, seq_length=10, lr=0.001, clip=5, val_frac=):\u001b[0m\n\u001b[1;37m                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Defining the training\n",
    "\n",
    "def train(model, data, epochs=10, batch_size=10, seq_length=10, lr=0.001, clip=5, val_frac=):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # creating a validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    n_chars = len(model.chars)\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        #initialize hidden state\n",
    "        h = model.ini_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # one hot encode the data and transform the data to tensors\n",
    "            x = one_hot_encoder(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            in train_on_gpu:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            # making sure that we detach any past and hidden state from its history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero out accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = model(inputs, h)\n",
    "            \n",
    "            # calc the loss and perform backpropagation\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip_grad_norm helps prevent the exploding gradient problem is RNNs/ LSTMs\n",
    "            # in these types of models, the gradients can become very big so\n",
    "            # we set a clip treshold and if the gradient is larger than that treshold we set it equal to the treshold\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # validation\n",
    "            if counter % print_every == 0:\n",
    "                # get validation loss\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # one hot encode\n",
    "                    x = one_hot_encoder(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # creating new vars for the hidden state\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if train_on_gpu:\n",
    "                        inputs, targets = inputs.cuda, targets.cuda()\n",
    "                        \n",
    "                    output, val_h = model(inputs, val, h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                    \n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                     \"Step: {}...\".format(counter),\n",
    "                     \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                     \"Val Loss: (:.4f)\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
